{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: credentialsFile\n",
      "Warning: Ignoring non-Spark config property: parentProject\n",
      "23/12/12 04:40:07 WARN Utils: Your hostname, codespaces-05ff3a resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/12/12 04:40:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/12 04:40:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .appName('arrival_ml')\n",
    "                     .config('parentProject', 'cta-tracking')\n",
    "                     .config('spark.jars', 'https://storage.googleapis.com/spark-lib/bigquery/spark-3.4-bigquery-0.34.0.jar')\n",
    "                     .config('credentialsFile', 'cta-tracking-6135a30a5c0c.json')\n",
    "                     .getOrCreate())\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str):\n",
    "    'read csv for development, may change to BQ in the future'\n",
    "    _schema = StructType([\n",
    "        StructField(\"destNm\", StringType(), nullable=True),\n",
    "        StructField(\"nextStaNm\", StringType(), nullable=True),\n",
    "        StructField(\"arrT\", TimestampType(), nullable=True),\n",
    "        StructField(\"isApp\", IntegerType(), nullable=True),\n",
    "        StructField(\"isDly\", IntegerType(), nullable=True),\n",
    "    ])\n",
    "    return (spark.read\n",
    "            .option('timestampFormat', 'MM/dd/yyyy HH:mm:ss')\n",
    "            .schema(_schema)\n",
    "            .csv(file_path, header=True)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bq_table(spark):\n",
    "      '''load last 2 months of data'''\n",
    "      spark.conf.set(\"viewsEnabled\",\"true\")\n",
    "      spark.conf.set(\"materializationDataset\",\"line_stops\")\n",
    "\n",
    "      sql_statement = '''\n",
    "            SELECT destNM, nextStaNm, arrT, isApp, isDly\n",
    "            FROM `cta-tracking.line_stops.blue`\n",
    "            WHERE resp_time >= DATE_SUB(CURRENT_DATE, INTERVAL 2 MONTH)\n",
    "      '''\n",
    "      df = (spark.read\n",
    "            .format('bigquery')\n",
    "            .option('query', sql_statement)\n",
    "            .load()\n",
    "      )\n",
    "      df = df.withColumn('arrT', func.col('arrT').cast(TimestampType()))\n",
    "      return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- destNM: string (nullable = true)\n",
      " |-- nextStaNm: string (nullable = true)\n",
      " |-- arrT: timestamp (nullable = true)\n",
      " |-- isApp: string (nullable = true)\n",
      " |-- isDly: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_bq_table(spark=spark)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrival_morning(raw_data: DataFrame,\n",
    "                    stop: str = 'Division')-> DataFrame:\n",
    "    '''split raw data into morning and afternoon section\n",
    "        param:\n",
    "            raw_data: initial data from data load\n",
    "            stop: name of the stop\n",
    "        return:\n",
    "            a splitted dataframe of interest\n",
    "    '''\n",
    "    df_approach = raw_data.filter(\n",
    "                    # spark is verbose\n",
    "                    # morning\n",
    "                    (func.hour(func.col('arrT'))<10) &\n",
    "                    (func.col('isApp')==1) &\n",
    "                    # train is approaching Division\n",
    "                    (func.col('nextStaNm')==stop) &\n",
    "                    # direction is towards to forest park or UIC\n",
    "                    ((func.col('destNm')=='Forest Park') | (func.col('destNM')=='UIC-Halsted'))\n",
    "                )\n",
    "    df_approach = df_approach.sort('arrT')\n",
    "\n",
    "    return df_approach\n",
    "\n",
    "df_morning = arrival_morning(raw_data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------------------+-----+-----+-------------------+------------+\n",
      "|     destNM|nextStaNm|               arrT|isApp|isDly|          next_arrT|arrival_diff|\n",
      "+-----------+---------+-------------------+-----+-----+-------------------+------------+\n",
      "|Forest Park| Division|2023-10-12 08:12:54|    1|    0|2023-10-12 08:21:13|         499|\n",
      "|Forest Park| Division|2023-10-12 08:21:13|    1|    0|2023-10-12 08:25:13|         240|\n",
      "|Forest Park| Division|2023-10-12 08:25:13|    1|    0|2023-10-12 08:26:54|         101|\n",
      "|Forest Park| Division|2023-10-12 08:26:54|    1|    0|2023-10-12 08:47:16|        1222|\n",
      "|Forest Park| Division|2023-10-12 08:47:16|    1|    0|2023-10-12 09:09:16|        1320|\n",
      "|Forest Park| Division|2023-10-12 09:09:16|    1|    0|2023-10-12 09:11:16|         120|\n",
      "|Forest Park| Division|2023-10-12 09:11:16|    1|    0|2023-10-12 09:31:12|        1196|\n",
      "|Forest Park| Division|2023-10-12 09:31:12|    1|    0|2023-10-12 09:37:14|         362|\n",
      "|Forest Park| Division|2023-10-12 09:37:14|    1|    0|2023-10-12 09:49:17|         723|\n",
      "|Forest Park| Division|2023-10-12 09:49:17|    1|    0|2023-10-12 09:55:09|         352|\n",
      "|Forest Park| Division|2023-10-12 09:55:09|    1|    0|2023-10-12 09:58:59|         230|\n",
      "|Forest Park| Division|2023-10-18 08:07:16|    1|    0|2023-10-18 08:08:52|          96|\n",
      "|Forest Park| Division|2023-10-18 08:08:52|    1|    0|2023-10-18 08:41:09|        1937|\n",
      "|Forest Park| Division|2023-10-18 08:41:09|    1|    0|2023-10-18 08:45:12|         243|\n",
      "|Forest Park| Division|2023-10-18 08:45:12|    1|    0|2023-10-18 08:56:54|         702|\n",
      "|Forest Park| Division|2023-10-18 08:56:54|    1|    0|2023-10-18 09:05:17|         503|\n",
      "|Forest Park| Division|2023-10-18 09:05:17|    1|    0|2023-10-18 09:11:16|         359|\n",
      "|Forest Park| Division|2023-10-18 09:11:16|    1|    0|2023-10-18 09:19:08|         472|\n",
      "|Forest Park| Division|2023-10-18 09:19:08|    1|    0|2023-10-18 09:26:58|         470|\n",
      "|Forest Park| Division|2023-10-18 09:26:58|    1|    0|2023-10-18 09:37:11|         613|\n",
      "+-----------+---------+-------------------+-----+-----+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def time_between_arrival(arrival: DataFrame\n",
    "                         )-> DataFrame:\n",
    "    windowSpec = Window().orderBy(\"arrT\").partitionBy(func.date_format(\"arrT\", \"yyyy-MM-dd\"))\n",
    "    df_arrival = arrival.withColumn('next_arrT', func.lead('arrT').over(windowSpec))\n",
    "    \n",
    "    df_arrival = df_arrival.withColumn('arrival_diff', \n",
    "                                       func.when(func.col('next_arrT').isNotNull(),\n",
    "                                                (func.col(\"next_arrT\").cast(\"long\") - func.col(\"arrT\").cast(\"long\"))\n",
    "                                                ).otherwise(None))\n",
    "    \n",
    "    df_arrival = df_arrival.dropna(subset='arrival_diff')\n",
    "    \n",
    "    return df_arrival\n",
    "\n",
    "time_between_arrival(arrival=df_morning).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cta_gcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
