{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/05 06:19:15 WARN Utils: Your hostname, codespaces-05ff3a resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/12/05 06:19:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/05 06:19:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('arrival_ml').getOrCreate()\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str):\n",
    "    'read csv for development, may change to BQ in the future'\n",
    "    _schema = StructType([\n",
    "        StructField(\"rn\", IntegerType(), nullable=True),\n",
    "        StructField(\"destSt\", IntegerType(), nullable=True),\n",
    "        StructField(\"destNm\", StringType(), nullable=True),\n",
    "        StructField(\"trDr\", IntegerType(), nullable=True),\n",
    "        StructField(\"nextStaId\", IntegerType(), nullable=True),\n",
    "        StructField(\"nextStpId\", IntegerType(), nullable=True),\n",
    "        StructField(\"nextStaNm\", StringType(), nullable=True),\n",
    "        StructField(\"prdt\", TimestampType(), nullable=True),\n",
    "        StructField(\"arrT\", TimestampType(), nullable=True),\n",
    "        StructField(\"flags\", StringType(), nullable=True),\n",
    "        StructField(\"lat\", StringType(), nullable=True),\n",
    "        StructField(\"lon\", StringType(), nullable=True),\n",
    "        StructField(\"heading\", IntegerType(), nullable=True),\n",
    "        StructField(\"resp_time\", TimestampType(), nullable=True),\n",
    "        StructField(\"isApp\", IntegerType(), nullable=True),\n",
    "        StructField(\"isDly\", IntegerType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    return (spark.read\n",
    "            .option('timestampFormat', 'MM/dd/yyyy HH:mm:ss')\n",
    "            .schema(_schema)\n",
    "            .csv(file_path, header=True)\n",
    "            )\n",
    "\n",
    "df = load_data(file_path='blue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               arrT|\n",
      "+-------------------+\n",
      "|2023-02-15 08:00:52|\n",
      "|2023-02-15 08:08:52|\n",
      "|2023-02-15 08:20:55|\n",
      "|2023-02-15 08:26:56|\n",
      "|2023-02-15 08:30:52|\n",
      "|2023-02-15 08:53:16|\n",
      "|2023-02-15 09:10:55|\n",
      "|2023-02-15 09:12:53|\n",
      "|2023-02-15 09:20:54|\n",
      "|2023-02-16 08:02:59|\n",
      "|2023-02-16 08:08:54|\n",
      "|2023-02-16 08:30:51|\n",
      "|2023-02-16 09:00:56|\n",
      "|2023-02-16 09:14:50|\n",
      "|2023-02-16 09:40:53|\n",
      "|2023-02-16 09:54:56|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def arrival_morning(raw_data: DataFrame,\n",
    "                    stop: str = 'Division')-> DataFrame:\n",
    "    '''split raw data into morning and afternoon section\n",
    "        param:\n",
    "            raw_data: initial data from data load\n",
    "            stop: name of the stop\n",
    "        return:\n",
    "            a splitted dataframe of interest\n",
    "    '''\n",
    "    df_approach = raw_data.filter(\n",
    "                    # spark is verbose\n",
    "                    # morning\n",
    "                    (func.hour(func.col('arrT'))<10) &\n",
    "                    (func.col('isApp')==1) &\n",
    "                    # train is approaching Division\n",
    "                    (func.col('nextStaNm')==stop) &\n",
    "                    # direction is towards to forest park or UIC\n",
    "                    ((func.col('destNm')=='Forest Park') | (func.col('destNM')=='UIC-Halsted'))\n",
    "                )\n",
    "    df_approach = df_approach.sort('arrT')\n",
    "\n",
    "    return df_approach\n",
    "\n",
    "df_morning = arrival_morning(raw_data=df)\n",
    "df_morning.select('arrT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+\n",
      "|               arrT|          next_arrT|arrival_diff|\n",
      "+-------------------+-------------------+------------+\n",
      "|2023-02-15 08:00:52|2023-02-15 08:08:52|         480|\n",
      "|2023-02-15 08:08:52|2023-02-15 08:20:55|         723|\n",
      "|2023-02-15 08:20:55|2023-02-15 08:26:56|         361|\n",
      "|2023-02-15 08:26:56|2023-02-15 08:30:52|         236|\n",
      "|2023-02-15 08:30:52|2023-02-15 08:53:16|        1344|\n",
      "|2023-02-15 08:53:16|2023-02-15 09:10:55|        1059|\n",
      "|2023-02-15 09:10:55|2023-02-15 09:12:53|         118|\n",
      "|2023-02-15 09:12:53|2023-02-15 09:20:54|         481|\n",
      "|2023-02-16 08:02:59|2023-02-16 08:08:54|         355|\n",
      "|2023-02-16 08:08:54|2023-02-16 08:30:51|        1317|\n",
      "|2023-02-16 08:30:51|2023-02-16 09:00:56|        1805|\n",
      "|2023-02-16 09:00:56|2023-02-16 09:14:50|         834|\n",
      "|2023-02-16 09:14:50|2023-02-16 09:40:53|        1563|\n",
      "|2023-02-16 09:40:53|2023-02-16 09:54:56|         843|\n",
      "+-------------------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def time_between_arrival(arrival: DataFrame\n",
    "                         )-> DataFrame:\n",
    "    windowSpec = Window().orderBy(\"arrT\").partitionBy(func.date_format(\"arrT\", \"yyyy-MM-dd\"))\n",
    "    df_arrival = arrival.withColumn('next_arrT', func.lead('arrT').over(windowSpec))\n",
    "    \n",
    "    df_arrival = df_arrival.withColumn('arrival_diff', \n",
    "                                       func.when(func.col('next_arrT').isNotNull(),\n",
    "                                                (func.col(\"next_arrT\").cast(\"long\") - func.col(\"arrT\").cast(\"long\"))\n",
    "                                                ).otherwise(None))\n",
    "    \n",
    "    df_arrival = df_arrival.dropna(subset='arrival_diff')\n",
    "    \n",
    "    return df_arrival\n",
    "\n",
    "time_between_arrival(arrival=df_morning).select('arrT', 'next_arrT', 'arrival_diff').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cta_gcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
